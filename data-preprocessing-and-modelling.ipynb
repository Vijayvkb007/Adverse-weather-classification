{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7801384,"sourceType":"datasetVersion","datasetId":4473715}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SYSTEM OF INTEREST (SOI) \n<ol><li>DATA PREPARATION</li>\n    <li>LEARNING MODEL</li>\n    <li>EVALUATION AND DEPLOYMENT</li></ol>","metadata":{}},{"cell_type":"markdown","source":"## DATA PREPARATION SUBSYSTEM","metadata":{}},{"cell_type":"markdown","source":"<h3><em>Data preparation used in the base paper is mentioned below:</em></h3>\n<ul><li><p>combined two main and common weather conditions datasets, DAWM2020 dataset and MCWCD2018,<br> to end up with a dataset composed of 1656 image samples that are grouped into six classes for weather conditions:<br> cloudy (300 images), rainy (215 images), snowy (204 images), sandy (319 images), shine (253 images), and sunrise (365 images)</p></li>\n<li><p>Image datasets are collected, combined, and labeled into six class labels by means of six folders, each of which holds the name of one weather class</p></li>\n<li><p>Initially, the image-type of all images is unified to JPG image extension, and then the image-resize operation is applied <br> over all JPG images\nin which the sizes of all images are converted to 3D matrices (RGB images) with image\ndimensions of 224 ×224 ×3.</p></li>\n<li><p>applying randomized augmentation operations on the dataset. The augmentation process configures <br>a set of preprocessing\noptions such as resizing, cropping, rotation, reflection, invariant distortions, and others.</p></li>\n<li><p>images are shuffled randomly.</p></li></ul>","metadata":{}},{"cell_type":"markdown","source":"<h3><em>Extra techniques used:</em></h3>\n<ul><li><p>Categorical labels are converted into numerical format for better compatibality for ML and DL algos befor feeding it into model.</p></li>\n<li><p>Normalization to ensure the data distribution is well-suited for the model's initial weights and activations and helps to converge fast.</p></li>\n<li><p>Data balancing, as the dataset is imbalanced.</p></li><ul>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torchvision import transforms\nimport random\nimport shutil\nfrom PIL import Image\nimport numpy as np\nimport os\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:01:43.920768Z","iopub.execute_input":"2024-03-10T13:01:43.921124Z","iopub.status.idle":"2024-03-10T13:01:50.419532Z","shell.execute_reply.started":"2024-03-10T13:01:43.921095Z","shell.execute_reply":"2024-03-10T13:01:50.418681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folders = list()\npath = \"/kaggle/input/ds-dataset/datasets\"\nfor folder in os.listdir(path):\n    folder = os.path.join(path, str(folder))\n    folders.append(os.path.join(*folder.split(\"/\")[3:]))\nprint(folders)\nfor folder in folders:    \n    os.makedirs(\"/kaggle/working/\"+folder, exist_ok=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:01:50.421091Z","iopub.execute_input":"2024-03-10T13:01:50.421481Z","iopub.status.idle":"2024-03-10T13:01:50.433437Z","shell.execute_reply.started":"2024-03-10T13:01:50.421456Z","shell.execute_reply":"2024-03-10T13:01:50.432640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_n_create(image_folder: str, output_folder:str) -> None:\n    \"\"\"\n    This function will take in image folder path as input\n    and stores the transformed images into differnt folder\n    \n    Here Normalization is not done because it hinders the image content\n    \"\"\"\n    transformations = transforms.Compose([\n    transforms.Resize((224, 224)), # transforming the dimensions \n    transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalizing the images\n        ])\n    \n    for folder in os.listdir(image_folder):\n        print(folder)\n        ifolder = os.path.join(image_folder, str(folder))\n        for files in os.listdir(ifolder):\n            ofiles = os.path.join(output_folder, str(folder), str(files))\n            ifiles = os.path.join(ifolder, str(files))\n            image = Image.open(ifiles)\n            # drop images which doesn't have rgb channels \n            if len(image.getbands()) == 3:\n                resized_img = transformations(image)\n                resized_img = transforms.ToPILImage()(resized_img)\n                resized_img.save(ofiles)\n\ntransform_n_create(\"/kaggle/input/ds-dataset/datasets/\", \"/kaggle/working/ds-dataset/datasets/\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:01:50.434489Z","iopub.execute_input":"2024-03-10T13:01:50.434761Z","iopub.status.idle":"2024-03-10T13:02:22.165126Z","shell.execute_reply.started":"2024-03-10T13:01:50.434739Z","shell.execute_reply":"2024-03-10T13:02:22.164364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augmentation(image_folder: str, output_folder:str) -> None:\n    \"\"\"\n    This function will take in image folder path as input\n    and stores the augmented images into differnt folder\n    \n    Here Normalization is not done because it hinders the image content\n    \"\"\"\n    data_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),  # Resize the image to 256x256\n    transforms.RandomRotation(degrees=15),  # Randomly rotate the image within +/- 15 degrees\n    transforms.CenterCrop(224),  # Randomly crop a 224x224 region from the resized image\n    transforms.RandomHorizontalFlip(p=1.0),  # Randomly flip the image horizontally\n#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust brightness, contrast, saturation, and hue\n    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n    ])\n    \n    for folder in os.listdir(image_folder):\n        print(folder)\n        ifolder = os.path.join(image_folder, str(folder))\n        for files in os.listdir(ifolder):\n            ofiles = os.path.join(output_folder, str(folder), str(files))\n            ifiles = os.path.join(ifolder, str(files))\n            image = Image.open(ifiles)\n            # drop images which doesn't have rgb channels \n            if len(image.getbands()) == 3:\n                resized_img = data_transforms(image)\n                resized_img = transforms.ToPILImage()(resized_img)\n                temp = ofiles.split(\".\")\n                newfile = temp[0] + \"a.\" + temp[1]\n                resized_img.save(newfile)\n\naugmentation(\"/kaggle/input/ds-dataset/datasets/\", \"/kaggle/working/ds-dataset/datasets/\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:22.167821Z","iopub.execute_input":"2024-03-10T13:02:22.168317Z","iopub.status.idle":"2024-03-10T13:02:46.506786Z","shell.execute_reply.started":"2024-03-10T13:02:22.168283Z","shell.execute_reply":"2024-03-10T13:02:46.505954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for shape changes in dataset\nimg_default = cv2.imread(\"/kaggle/input/ds-dataset/datasets/Cloudy/cloudy116.jpg\")\nimg_augmented = cv2.imread(\"/kaggle/working/ds-dataset/datasets/Cloudy/cloudy116a.jpg\")\n\nprint(f\"Shape of normal image {img_default.shape}\")\nprint(f\"Shape of augmented image {img_augmented.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:46.507903Z","iopub.execute_input":"2024-03-10T13:02:46.508483Z","iopub.status.idle":"2024-03-10T13:02:46.546777Z","shell.execute_reply.started":"2024-03-10T13:02:46.508450Z","shell.execute_reply":"2024-03-10T13:02:46.545879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to create train,test and validation sets \npath = \"/kaggle/working/ds-dataset/datasets\"\nfor fold_type in [\"train_data\", \"test_data\", \"val_data\"]:\n    folders = list()\n    for folder in os.listdir(path):\n        folder = os.path.join(path, str(folder))\n        folder = folder.split(\"/\")[3:]\n        folders.append(os.path.join(folder[0], fold_type, folder[2]))\n    print(folders)\n    for folder in folders:    \n        os.makedirs(\"/kaggle/working/\"+folder, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:46.547745Z","iopub.execute_input":"2024-03-10T13:02:46.548031Z","iopub.status.idle":"2024-03-10T13:02:46.556596Z","shell.execute_reply.started":"2024-03-10T13:02:46.547977Z","shell.execute_reply":"2024-03-10T13:02:46.555634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select random images and add it to folders train and test\ntrain_path = \"/kaggle/working/ds-dataset/train_data\"\ntest_path = \"/kaggle/working/ds-dataset/test_data\"\nfor folder in os.listdir(path):\n    foldpath = os.path.join(path, str(folder))\n    comp_files = os.listdir(foldpath)\n#     print(comp_files)\n    filenames = random.sample(comp_files, int(0.8*len(comp_files)))\n    left_files = list(set(comp_files) - set(filenames))\n    \n    trainpath = os.path.join(train_path, str(folder))\n    testpath = os.path.join(test_path, str(folder))\n    \n    for file in filenames:\n        srcpath = os.path.join(foldpath, file)\n        shutil.copy(srcpath, trainpath)\n    for file in left_files:\n        srcpath = os.path.join(foldpath, file)\n        shutil.copy(srcpath, testpath)\n        \ntrain_path = \"/kaggle/working/ds-dataset/train_data\"\nval_path = \"/kaggle/working/ds-dataset/val_data\"\nfor folder in os.listdir(train_path):\n    foldpath = os.path.join(train_path, str(folder))\n    comp_files = os.listdir(foldpath)\n#     print(comp_files)\n    filenames = random.sample(comp_files, int(0.2*len(comp_files)))\n    \n    valpath = os.path.join(val_path, str(folder))\n    \n    for file in filenames:\n        srcpath = os.path.join(foldpath, file)\n        shutil.move(srcpath, valpath)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:46.557712Z","iopub.execute_input":"2024-03-10T13:02:46.558076Z","iopub.status.idle":"2024-03-10T13:02:47.074178Z","shell.execute_reply.started":"2024-03-10T13:02:46.558052Z","shell.execute_reply":"2024-03-10T13:02:47.073199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image.open(\"/kaggle/input/ds-dataset/datasets/Cloudy/cloudy116.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:47.075340Z","iopub.execute_input":"2024-03-10T13:02:47.075616Z","iopub.status.idle":"2024-03-10T13:02:47.598921Z","shell.execute_reply.started":"2024-03-10T13:02:47.075593Z","shell.execute_reply":"2024-03-10T13:02:47.597926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image.open(\"/kaggle/working/ds-dataset/datasets/Cloudy/cloudy116.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:47.600167Z","iopub.execute_input":"2024-03-10T13:02:47.600439Z","iopub.status.idle":"2024-03-10T13:02:47.631798Z","shell.execute_reply.started":"2024-03-10T13:02:47.600417Z","shell.execute_reply":"2024-03-10T13:02:47.630892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image.open(\"/kaggle/working/ds-dataset/datasets/Cloudy/cloudy116a.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:47.635775Z","iopub.execute_input":"2024-03-10T13:02:47.636082Z","iopub.status.idle":"2024-03-10T13:02:47.668191Z","shell.execute_reply.started":"2024-03-10T13:02:47.636058Z","shell.execute_reply":"2024-03-10T13:02:47.667282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LEARNING MODEL SUBSYSTEM","metadata":{}},{"cell_type":"markdown","source":"<h3><em>Input Layer:</em></h3>\n<p>RGB images of dimensions 224x224x3 passed as input for CNNs.</p>\n\n<h3><em>Processing Layer:</em></h3>\n<h4><em>Transfer learning on three deep CNNs used in the base paper:</em></h4>\n<ul>\n    <li><p>SqueezeNet CNN</p></li>\n    <li><p> ResNet-50 CNN</p></li>\n    <li><p>EfficientNet-b0 CNN</p></li>\n</ul>\n<p>The model is\nfully trained to do classification task A. The knowledge (pretrained parameters)<br> is\nstored and transferred to the new model to do classification task B with fine-tuning.</p>\n\n<h3><em>Output Layer:</em></h3>\n<p>The output of SqueezeNet (1000) is fully connected with the number\nof classes (6), the output of ResNet-50 (2048)<br> is fully connected with the number of\nclasses (6), and the output of EfficientNet-B0 (1280) is fully connected with the<br> number\nof classes (6). The final output will be provided as a SoftMax probability function, and\nthe maximum<br> probability will be selected to represent the final classification result.\n</p>\n<!-- ![image.png](\"https://drive.usercontent.google.com/download?id=18Lz39CM7Qp7acZe2zq0KQDPhO0Q9jKnY\")\n<img src=\"https://drive.usercontent.google.com/download?id=18Lz39CM7Qp7acZe2zq0KQDPhO0Q9jKnY\"> -->\n\n<h3><em>Changes:</em></h3>\n<p>\n    Training the model for 7 classes which include,\n    <ul>\n        <li>Cloudy\n        <li>Snow\n        <li>Sand\n        <li>Sunrise\n        <li>Fog\n        <li>Shine\n        <li>Rainy\n    </ul>\n<p>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:47.669265Z","iopub.execute_input":"2024-03-10T13:02:47.669554Z","iopub.status.idle":"2024-03-10T13:02:58.496421Z","shell.execute_reply.started":"2024-03-10T13:02:47.669529Z","shell.execute_reply":"2024-03-10T13:02:58.495566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 7","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:58.497611Z","iopub.execute_input":"2024-03-10T13:02:58.498181Z","iopub.status.idle":"2024-03-10T13:02:58.502531Z","shell.execute_reply.started":"2024-03-10T13:02:58.498154Z","shell.execute_reply":"2024-03-10T13:02:58.501470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading pre-trained efficientnetB0 model , discarding the top layers \neffi_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:58.503876Z","iopub.execute_input":"2024-03-10T13:02:58.504262Z","iopub.status.idle":"2024-03-10T13:03:01.758810Z","shell.execute_reply.started":"2024-03-10T13:02:58.504229Z","shell.execute_reply":"2024-03-10T13:03:01.758046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeze the base model layers \neffi_model.trainable = False","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:01.759829Z","iopub.execute_input":"2024-03-10T13:03:01.760112Z","iopub.status.idle":"2024-03-10T13:03:01.769765Z","shell.execute_reply.started":"2024-03-10T13:03:01.760089Z","shell.execute_reply":"2024-03-10T13:03:01.768930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add custom layers for classification\nx = effi_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = Dense(1280, activation=\"relu\")(x)\npredictions = Dense(num_classes, activation=\"softmax\")(x)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:01.770952Z","iopub.execute_input":"2024-03-10T13:03:01.771319Z","iopub.status.idle":"2024-03-10T13:03:01.804707Z","shell.execute_reply.started":"2024-03-10T13:03:01.771287Z","shell.execute_reply":"2024-03-10T13:03:01.804039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the model\nmodel = Model(inputs=effi_model.input, outputs=predictions)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:01.805765Z","iopub.execute_input":"2024-03-10T13:03:01.806053Z","iopub.status.idle":"2024-03-10T13:03:01.842137Z","shell.execute_reply.started":"2024-03-10T13:03:01.806025Z","shell.execute_reply":"2024-03-10T13:03:01.841052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:01.843484Z","iopub.execute_input":"2024-03-10T13:03:01.843865Z","iopub.status.idle":"2024-03-10T13:03:01.859383Z","shell.execute_reply.started":"2024-03-10T13:03:01.843836Z","shell.execute_reply":"2024-03-10T13:03:01.858621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display summary \nmodel.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-10T13:03:01.860676Z","iopub.execute_input":"2024-03-10T13:03:01.861120Z","iopub.status.idle":"2024-03-10T13:03:02.178667Z","shell.execute_reply.started":"2024-03-10T13:03:01.861087Z","shell.execute_reply":"2024-03-10T13:03:02.177774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><strong>NOTE :</strong> The tensorflow data generator will convert the categorical labels into numerical labels, so no need of explicit conversion</p>","metadata":{}},{"cell_type":"code","source":"# convert the files into train, test, and validation sets\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/working/ds-dataset/train_data\", image_size=(224, 224),\n    batch_size=32\n    )\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/working/ds-dataset/test_data\", image_size=(224, 224),\n    batch_size=32\n    )\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/working/ds-dataset/val_data\", image_size=(224, 224),\n    batch_size=32\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:02.179931Z","iopub.execute_input":"2024-03-10T13:03:02.180242Z","iopub.status.idle":"2024-03-10T13:03:04.298858Z","shell.execute_reply.started":"2024-03-10T13:03:02.180218Z","shell.execute_reply":"2024-03-10T13:03:04.298143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=30\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:04.299882Z","iopub.execute_input":"2024-03-10T13:03:04.300155Z","iopub.status.idle":"2024-03-10T13:05:30.368381Z","shell.execute_reply.started":"2024-03-10T13:03:04.300121Z","shell.execute_reply":"2024-03-10T13:05:30.367448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(test_ds)\n\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:05:30.369442Z","iopub.execute_input":"2024-03-10T13:05:30.369704Z","iopub.status.idle":"2024-03-10T13:05:37.844629Z","shell.execute_reply.started":"2024-03-10T13:05:30.369682Z","shell.execute_reply":"2024-03-10T13:05:37.843834Z"},"trusted":true},"execution_count":null,"outputs":[]}]}